{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\"> Text Preprocessing: Python and Bash </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/skyclear.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is all about leveraging tools, techniques and algorithms to process and understand natural language-based data, which is usually unstructured like text, speech and so on.\n",
    "Computers are great at working with structured data like spreadsheets and database tables. But us humans usually communicate in words, not in tables. That’s unfortunate for computers.\n",
    "A lot of information in the world is unstructured — raw text in English or another human language. How can we get a computer to understand unstructured text and extract data from it?\n",
    "\n",
    "First step is to preprocess and clean our raw text data. This also depends on our final task.\n",
    "\n",
    "After the preprocessing phase we will transform our text from human language to machine-readable format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp_pipeline.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.18'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "#import textacy\n",
    "import string\n",
    "# If not already installed\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "#!python -m spacy download it_core_news_sm\n",
    "#!pip3 install -U textacy\n",
    "spacy.about.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm\n",
    "#!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" markdown=\"1\"> Playing with strings in Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations with Strings\n",
    "\n",
    "Stings are like list and thus they have a length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = [1,2,3,4,5,'c']\n",
    "ll[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e    ay that Banksy\n",
      "How long is our string? 22\n"
     ]
    }
   ],
   "source": [
    "ex1 = 'They say that Banksy is Robin Gunningham'\n",
    "ex2 = 'where\\'s the revolution'\n",
    "\n",
    "print(f'{ex1[2]}    {ex1[6:20]}')\n",
    "print('How long is our string?', len(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They say that Banksy is Robin Gunningham ciao\n"
     ]
    }
   ],
   "source": [
    "print(ex1 + ' ' + 'ciao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxx:  They say that Banksy is Robin Gunningham\n"
     ]
    }
   ],
   "source": [
    "print('xxxxxx: ', ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxx: They say that Banksy is Robin Gunningham\n"
     ]
    }
   ],
   "source": [
    "print('xxxxxx: %s' %(ex1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know that you can add strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Feynman\n",
      "RichardRichardRichard\n"
     ]
    }
   ],
   "source": [
    "name, surname ='Richard', 'Feynman'\n",
    "print(name + ' ' + surname)\n",
    "print(name*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La Vita È Bella'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = 'lA viTa è beLla'\n",
    "title.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are lost..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "findit = 'where is my mind'\n",
    "print(findit.find('mind'))\n",
    "print(findit.index('mind'))\n",
    "print(findit.find('kiwi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findit.endswith('mind'), findit.startswith('mind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cara Marina, sei l'unica al mondo\n",
      "\n",
      "Cara Giulia, sei l'unica al mondo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dedica = 'Cara Marina, sei l\\'unica al mondo'\n",
    "print(dedica + '\\n')\n",
    "dedica = dedica.replace('Marina', 'Giulia')\n",
    "print(dedica + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Take away ', 'it', '')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part = 'Take away it'\n",
    "part.partition('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of pi is 3.14159\n",
      "The value of pi is 3.14159\n",
      "The value of pi is 3.1416\n",
      "Flight Number: AZA86E. Flight Number: AZA151.\n"
     ]
    }
   ],
   "source": [
    "pi = 3.14159\n",
    "method1 = \"The value of pi is \" + str(pi)\n",
    "method2 = \"The value of pi is {}\".format(pi)\n",
    "method3 = \"The value of pi is {0:.4f}\".format(pi)\n",
    "print(method1 + '\\n' + method2 + '\\n' + method3)\n",
    "method4 = \"\"\"Flight Number: {0}. Flight Number: {1}.\"\"\".format('AZA86E', 'AZA151')\n",
    "print(method4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PUT IT UPPER'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'put it upper'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strip it'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'    strip it  '.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    strip it  '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tannutuva    strip it  tannutuva'.strip('tannutuva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['more', 'is', 'better', 'than', 'less']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'more is better than less'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two', 'three', 'four', 'five']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'one,two,three,four,five'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Less is better than boh!?!'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['Less', 'is', 'better', 'than', 'boh!?!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Less , is , better , than , boh!?!'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' , '.join(['Less', 'is', 'better', 'than', 'boh!?!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions \n",
    "\n",
    "A regular expression, o regex, is a sequence of characters that define a search pattern. Some useful special characters:\n",
    "\n",
    "* **\"\\d\"**\tMatch any digit\t\t\n",
    "* **\"\\D\"**\tMatch any non-digit\n",
    "* **\"\\s\"**\tMatch any whitespace\t\t\n",
    "* **\"\\S\"**\tMatch any non-whitespace\n",
    "* **\"\\w\"**\tMatch any alphanumeric char\t\t\n",
    "* **\"\\W\"**\tMatch any non-alphanumeric char\n",
    "\n",
    "\n",
    "* **?**\tMatch zero or one repetitions of preceding:\t\"ab?\" matches \"a\" or \"ab\"\n",
    "* *****\tMatch zero or more repetitions of preceding:\t\"ab*\" matches \"a\", \"ab\", \"abb\", \"abbb\"...\n",
    "* **+**\tMatch one or more repetitions of preceding:\t\"ab+\" matches \"ab\", \"abb\", \"abbb\"... but not \"a\"\n",
    "* **{n}**\tMatch n repetitions of preeeding\t\"ab{2}\" matches \"abb\"\n",
    "* **{m,n}**\tMatch between m and n repetitions of preceding\t\"ab{2,3}\" matches \"abb\" or \"abbb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['more', 'is', 'better', 'than', '', '', '', '', '', 'less']\n",
      "['more', 'is', 'better', 'than', 'less']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "line = 'more is better than      less'\n",
    "regex1 = re.compile('\\s')\n",
    "regex2 = re.compile('\\s+')\n",
    "# \"\\s\" is a special character that matches any whitespace (space, tab, newline, etc.), \n",
    "# and the \"+\" is a character that indicates one or more of the entity preceding it\n",
    "print(regex1.split(line))\n",
    "print(regex2.split(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denobili16@gmail.com', 'denobili@gmail.com']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = re.compile('\\w+@\\w+\\.[a-z]{3}')\n",
    "mails = 'I have several mails. My personal mails are denobili16@gmail.com and cristiano.denobili@gmail.com'\n",
    "\n",
    "email.findall(mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denobili16@gmail.com', 'cristiano.denobili@gmail.com']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = re.compile(r'[\\w.]+@\\w+\\.[a-z]{3}')\n",
    "email.findall(mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p l', 'g i', 's c', 'l b', 't f', 'g i', 's c', 'r y', 'r n']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('\\w\\s\\w')\n",
    "# \\w is a special marker matching any alphanumeric character.\n",
    "regex.findall('deep learning is cool but flying is cooler y or n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you'd like to match any of these characters \n",
    "\n",
    ". ^ $ * + ? { } [ ] \\ | ( )\n",
    "\n",
    "you can escape them with a back-slash!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$', '$']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('\\$')\n",
    "regex.findall(\"the cost is $20, but you you jump twice is $15.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"text-align: center;\" markdown=\"1\"> Text Normalization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lower or upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n",
    "input_str_lower = input_str.lower()\n",
    "\n",
    "print(input_str_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tr in bash:\n",
    "\n",
    "```console\n",
    "$: cat text_to_norm.txt  | tr '[:upper:]' '[:lower:]'\n",
    "$: cat text_to_norm.txt  | tr '[:lower:]' '[:upper:]'\n",
    "\n",
    "or\n",
    "\n",
    "$: cat data/text_to_norm.txt | awk '{print tolower($0)}' \n",
    "$: cat data/text_to_norm.txt | awk '{print toupper($0)}'\n",
    "```\n",
    "\n",
    "If you want to time them:\n",
    "\n",
    "```console\n",
    "$: time for i in `seq 100000`; do cat text_to_norm.txt  | tr '[:upper:]' '[:lower:]' > /dev/null 2>&1; done\n",
    "\n",
    "or\n",
    "\n",
    "$: time for i in `seq 100000`; do cat data/text_to_norm.txt | awk '{print toupper($0)} > /dev/null 2>&1; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\r\n",
      "why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "la cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "is the universe @ quantum computer?\r\n",
      "la vita è meravigli!osa! senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat ./data/text_to_norm.txt  | tr '[:upper:]' '[:lower:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE 5 BIGGEST COUNTRIES BY POPULATION IN 2017 ARE CHINA, INDIA, UNITED STATES, INDONESIA, AND BRAZIL.\r\n",
      "WHY DO   MOSQUITOES EXIST?\r\n",
      "ONE TWO THREE 4 FIVE!\r\n",
      "LA COSA BELLA 56700 DI UNA BATTUTA A DOPPIO SENSO È CHE PUÒ SIGNIFICARE SOLO UNA COSA\r\n",
      "IS THE UNIVERSE @ QUANTUM COMPUTER?\r\n",
      "LA VITA È MERAVIGLI!OSA! SENZA & SARESTI MORTO...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat ./data/text_to_norm.txt  | tr '[:lower:]' '[:upper:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\r\n",
      "why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "la cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "is the universe @ quantum computer?\r\n",
      "la vita è meravigli!osa! senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/text_to_norm.txt | awk '{print tolower($0)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE 5 BIGGEST COUNTRIES BY POPULATION IN 2017 ARE CHINA, INDIA, UNITED STATES, INDONESIA, AND BRAZIL.\r\n",
      "WHY DO   MOSQUITOES EXIST?\r\n",
      "ONE TWO THREE 4 FIVE!\r\n",
      "LA COSA BELLA 56700 DI UNA BATTUTA A DOPPIO SENSO è CHE PUò SIGNIFICARE SOLO UNA COSA\r\n",
      "IS THE UNIVERSE @ QUANTUM COMPUTER?\r\n",
      "LA VITA è MERAVIGLI!OSA! SENZA & SARESTI MORTO...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/text_to_norm.txt | awk '{print toupper($0)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "real\t0m3.195s\r\n",
      "user\t0m2.322s\r\n",
      "sys\t0m2.998s\r\n"
     ]
    }
   ],
   "source": [
    "! time for i in `seq 1000`; do cat data/text_to_norm.txt  | tr '[:upper:]' '[:lower:]' > /dev/null 2>&1; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "real\t0m2.753s\r\n",
      "user\t0m2.104s\r\n",
      "sys\t0m2.780s\r\n"
     ]
    }
   ],
   "source": [
    "! time for i in `seq 1000`; do cat data/text_to_norm.txt | awk '{print toupper($0)}' > /dev/null 2>&1; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers\n",
    "\n",
    "[Regular Expression](https://docs.python.org/3.6/howto/regex.html)\n",
    "\n",
    "[GNU/Linux Command-Line Tools Summary](http://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+', '', input_str)\n",
    "#  \\d Matches any decimal digit; this is equivalent to the class [0-9].\n",
    "#  + which matches one or more times.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  biggest countries by population in  are China, India, United States, Indonesia, and Brazil.\r\n",
      "Why do   mosquitoes exist?\r\n",
      "one two three  five!\r\n",
      "La cosa bella  di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/text_to_norm.txt  | tr -d '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  biggest countries by population in  are China, India, United States, Indonesia, and Brazil.\r\n",
      "Why do   mosquitoes exist?\r\n",
      "one two three  five!\r\n",
      "La cosa bella  di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! sed -e 's/[0-9\\*]//g' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\r\n",
      "Why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "La cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! awk 'NF<=5' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\r\n",
      "La cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n"
     ]
    }
   ],
   "source": [
    "! awk 'NF>5' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grep (select) Specific Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\r\n",
      "Why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! grep '\\bcosa bella\\b' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\r\n"
     ]
    }
   ],
   "source": [
    "! grep '\\bIndia\\b' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "La cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe @ quantum computer?\r\n",
      "La vita è meravigli!osa! Senza & saresti morto...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! grep -v '\\bIndia\\b' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vita è meravigli!osa! Senza & saresti morto...\r\n"
     ]
    }
   ],
   "source": [
    "! grep -P '(\\bvita\\b).*(\\bmorto\\b)' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "\n",
    "The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n",
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "input_str = 'This &is [an] example? {of} string. with.? punctuation!!!!'\n",
    "\n",
    "# 1st method\n",
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "input_str_nopunct = input_str.translate(table)\n",
    "print(input_str_nopunct)\n",
    "\n",
    "# 2nd method: substitute every NON alpha-numeric char with space\n",
    "input_str_nopunct = re.sub(r'[^\\w\\s]','',input_str)\n",
    "print(input_str_nopunct)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China India United States Indonesia and Brazil\r\n",
      "Why do   mosquitoes exist\r\n",
      "one two three 4 five\r\n",
      "La cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe  quantum computer\r\n",
      "La vita è meravigliosa Senza  saresti morto\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/text_to_norm.txt | tr -d '[:punct:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China India United States Indonesia and Brazil\r\n",
      "Why do   mosquitoes exist\r\n",
      "one two three 4 five\r\n",
      "La cosa bella di una battuta a doppio senso � che può significare solo una cosa\r\n",
      "Is the Universe  quantum computer\r\n",
      "La vita � meravigliosa Senza  saresti morto\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! awk '{ gsub(/[[:punct:]]/, \"\", $0) }1;' data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 biggest countries by population in 2017 are China India United States Indonesia and Brazil\r\n",
      "Why do   mosquitoes exist?\r\n",
      "one two three 4 five!\r\n",
      "La cosa bella 56700 di una battuta a doppio senso è che può significare solo una cosa\r\n",
      "Is the Universe  quantum computer?\r\n",
      "La vita è meravigli!osa! Senza  saresti morto\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# [] identifies a range: m[a,u,o]m -> mam, mum, mom\n",
    "# [^] it performs a logical not\n",
    "! sed -e \"s/[^a-zA-Z0-9àèéìò\\!'?\\* ]//g\" data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,,,.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "@\r\n",
      "&...\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! sed -e \"s/[a-zA-Z0-9àèéìò\\!'?\\* ]//g\" data/text_to_norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't take to much       space\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'Don\\'t take to much       space'\n",
    "#regex1 = re.compile('\\s')\n",
    "re.sub(r'\\s+', ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't take to much space\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"don't take to much    space\" | sed 's/  */ /g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part of Speech Tagging & Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" markdown=\"1\"> Spacy: Python Library for Text Preprocessing </h2>\n",
    "\n",
    "\n",
    "[Spacy Official Page](https://spacy.io)\n",
    "\n",
    "**spaCy** is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages\n",
    "\n",
    "<h3 style=\"text-align: center;\" markdown=\"1\"> Language Models </h3>\n",
    "\n",
    "\n",
    "[Spacy Language Models Official Page](https://spacy.io/models/en)\n",
    "\n",
    "Using spaCy to extract linguistic features like part-of-speech tags, dependency labels and named entities, customising the tokenizer and working with the rule-based matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English: en_core_web_sm\n",
    "\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
    "\n",
    "'_sm' stands for small. You can download also medium and large version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm\n",
    "#!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "c\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "ll = ['a', 'c', 'c']\n",
    "\n",
    "for tokin in ll:\n",
    "    print(tokin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text**: The original word text.\n",
    "* **Lemma**: The base form of the word.\n",
    "* **POS**: The simple part-of-speech tag.\n",
    "* **Tag**: The detailed part-of-speech tag.\n",
    "* **Dep**: Syntactic dependency, i.e. the relation between tokens.\n",
    "* **Shape**: The word shape – capitalisation, punctuation, digits.\n",
    "* **is alpha**: Is the token an alpha character?\n",
    "* **is stop**: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time time NOUN NN nsubj Xxxx True False\n",
      "flies fly VERB VBZ ROOT xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Time flies')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging (POS)\n",
    "\n",
    "*Ci sei alle sei?*\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. \n",
    "\n",
    "part of speech: Noun (N), Verb (V), Adjective(ADJ), Adverb (ADV), Preposition (P), Conjunction (CON), Pronoun(PRO), Interjection (INT)\n",
    "\n",
    "After tokenization, spacy can parse and tag a given doc. This is where the statistical model comes in, which enables spacy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalise across the language. For instance, a word following \"the\" in English is most likely a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time time NOUN\n",
      "flies fly VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Time flies')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "journey NOUN\n",
      "of ADP\n",
      "a DET\n",
      "thousand NUM\n",
      "miles NOUN\n",
      "begins VERB\n",
      "with ADP\n",
      "one NUM\n",
      "step NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The journey of a thousand miles begins with one step.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"893-0\" class=\"displacy\" width=\"1800\" height=\"487.0\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">journey</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">thousand</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">miles</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">begins</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">one</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">step.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1100.0,2.0 1100.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-5\" stroke-width=\"2px\" d=\"M420,352.0 C420,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,354.0 L928.0,342.0 912.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260.0,354.0 L1268.0,342.0 1252.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-893-0-8\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-893-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,354.0 L1623.0,342.0 1607.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On jupyter, othervise use displacy.serve\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Let's built a lemmatizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She went home because she was tired\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-PRON- go home because -PRON- be tired'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'She went home because she was tired'\n",
    "print(text)\n",
    "\n",
    "def lemmatizer(corpus):\n",
    "    doc = nlp(corpus)\n",
    "    lmtz = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lmtz)\n",
    "\n",
    "lemmatizer('She went home because she was tired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and apply map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def marameo(x):\n",
    "    x_str = str(x)\n",
    "    x_str_up = x_str.upper()\n",
    "    return x_str_up\n",
    "\n",
    "marameo(banksy)\n",
    "\n",
    "marameo_lmd = lambda x: str(x).upper()\n",
    "\n",
    "marameo_lmd(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SHE WENT HOME BECAUSE SHE WAS TIRED',\n",
       " \"I DON'T WANT TO BE UNCOLORED\",\n",
       " 'SHE IS THE ONE']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['She went home because she was tired', 'I don\\'t want to be uncolored', 'She is the one']\n",
    "corpus_lemmatized = list(map(lambda x: x.upper(), corpus))\n",
    "corpus_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use now a lambda function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she went home because she was tired',\n",
       " \"i don't want to be uncolored\",\n",
       " 'she is the one']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.lower(), corpus_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products.\n",
    "A named entity is a \"real-world object\" that's assigned a name. For example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London (GPE : Countries, cities, states )\n",
      "England (GPE : Countries, cities, states )\n",
      "\n",
      " (GPE : Countries, cities, states )\n",
      "the United Kingdom (GPE : Countries, cities, states )\n",
      "  (ORDINAL : \"first\", \"second\", etc. )\n",
      "the River Thames (ORG : Companies, agencies, institutions, etc. )\n",
      "\n",
      " (GPE : Countries, cities, states )\n",
      "Great Britain (GPE : Countries, cities, states )\n",
      "London (GPE : Countries, cities, states )\n",
      "\n",
      " (GPE : Countries, cities, states )\n",
      "two (CARDINAL : Numerals that do not fall under another type )\n",
      "Romans (NORP : Nationalities or religious or political groups )\n",
      "Londinium (PERSON : People, including fictional )\n",
      "\n",
      " (GPE : Countries, cities, states )\n"
     ]
    }
   ],
   "source": [
    "# Load the large English NLP model\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)\n",
    "\n",
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    #print(spacy.explain(entity.label_))\n",
    "    spacy_expl=spacy.explain(entity.label_)\n",
    "    print(f\"{entity.text} ({entity.label_} : {spacy_expl} )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is the capital and most populous city of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the United Kingdom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "     \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       "Standing on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the River Thames\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in the south east \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "of the island of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Great Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " has been a major settlement \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " millennia. It was founded by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Romans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", who named it \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Londinium\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "#displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks\n",
    "\n",
    "a phrase or group of words which can be learnt as a unit by somebody who is learning a language. Examples of chunks are ‘Can I have the bill, please?’ and ‘Pleased to meet you’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and similarity\n",
    "\n",
    "<img src=\"images/wordembeding.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are **word embeddings** exactly? Loosely speaking, they are vector representations of a particular word. \n",
    "\n",
    "https://spacy.io/usage/vectors-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.53906965\n",
      "dog banana 0.28761008\n",
      "cat dog 0.53906965\n",
      "cat cat 1.0\n",
      "cat banana 0.48752153\n",
      "banana dog 0.28761008\n",
      "banana cat 0.48752153\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#import sputnik\n",
    "#sputnik.install('spacy', spacy.about.__version__, 'en_core_web_sm', data_path='~/my_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English: en_core_web_sm\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital and most populous city of England and the United Kingdom.\n",
      "London PROPN nsubj\n",
      "is VERB ROOT\n",
      "the DET det\n",
      "capital NOUN attr\n",
      "and CCONJ cc\n",
      "most ADV advmod\n",
      "populous ADJ amod\n",
      "city NOUN conj\n",
      "of ADP prep\n",
      "England PROPN pobj\n",
      "and CCONJ cc\n",
      "the DET det\n",
      "United PROPN compound\n",
      "Kingdom PROPN conj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and the United Kingdom.\"\"\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian: it_core_news_sm\n",
    "\n",
    "Assigns context-specific token vectors, POS tags, dependency parse and named entities. Supports identification of PER, LOC, ORG and MISC entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'aperitivo è una bevanda alcolica o analcolica che si beve prima dei pasti per stimolare l'appetito. \n",
      "Può essere un cocktail o una bevanda non miscelata accompagnata o meno a stuzzichini.\n",
      "L'aperitivo NOUN nsubj\n",
      "è VERB cop\n",
      "una DET det\n",
      "bevanda NOUN ROOT\n",
      "alcolica ADJ amod\n",
      "o CONJ cc\n",
      "analcolica ADJ conj\n",
      "che PRON nsubj\n",
      "si PRON expl:pass\n",
      "beve VERB acl:relcl\n",
      "prima ADV advmod\n",
      "dei DET det\n",
      "pasti NOUN nsubj:pass\n",
      "per ADP mark\n",
      "stimolare VERB advcl\n",
      "l'appetito VERB obj\n",
      ". PUNCT punct\n",
      "\n",
      " SPACE \n",
      "Può AUX aux\n",
      "essere VERB cop\n",
      "un DET det\n",
      "cocktail NOUN ROOT\n",
      "o CONJ cc\n",
      "una DET det\n",
      "bevanda NOUN conj\n",
      "non ADV advmod\n",
      "miscelata VERB amod\n",
      "accompagnata VERB acl\n",
      "o CONJ cc\n",
      "meno ADV conj\n",
      "a ADP case\n",
      "stuzzichini NOUN obl\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"L'aperitivo è una bevanda alcolica o analcolica che si beve prima dei pasti per stimolare l'appetito. \n",
    "Può essere un cocktail o una bevanda non miscelata accompagnata o meno a stuzzichini.\"\"\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ci sei alle sei?\n",
      "ci PRON expl\n",
      "sei NUM nummod\n",
      "alle NOUN ROOT\n",
      "sei NUM nsubj\n",
      "? PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "text = 'ci sei alle sei?'\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
