{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\"> Generate a Dataset with Pandas </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus_six_num: 103484, Corpus_six_ver: 285704, Ratio: 0.362\n",
      "CPU times: user 136 ms, sys: 7 ms, total: 143 ms\n",
      "Wall time: 270 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_lines_num  = sum(1 for line in open(ROOT_PATH + '/data/sei_6/num/sei_num'))\n",
    "num_lines_ver  = sum(1 for line in open(ROOT_PATH + '/data/sei_6/ver/sei_verb'))\n",
    "\n",
    "ratio = num_lines_num  / float(num_lines_ver) \n",
    "print('Corpus_six_num: %d, Corpus_six_ver: %d, Ratio: %0.3f' %(num_lines_num, num_lines_ver, ratio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Data-Frame (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_1, directory_2):\n",
    "    # NOTE: Put in directory_2 the largest corpus\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"class\"] = []\n",
    "    l1 = 0\n",
    "    for file_path in os.listdir(directory_1):\n",
    "        with tf.gfile.GFile(os.path.join(directory_1 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines()]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(1)\n",
    "    \n",
    "    for file_path in os.listdir(directory_2):\n",
    "        with tf.gfile.GFile(os.path.join(directory_2 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines() if np.random.random() <= ratio]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 s, sys: 23.2 ms, total: 1.33 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_1 = ROOT_PATH + '/data/sei_6/num/'\n",
    "directory_2 = ROOT_PATH + '/data/sei_6/ver/'\n",
    "\n",
    "dataset_df = load_dataset(directory_1, directory_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "class          \n",
       "0        103215\n",
       "1        103484"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Balanced\n",
    "dataset_df.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a al comma dopo le parole inferiore a sei\n",
      "tuttavia sono sei cifre\n",
      "a anni accoltellò all'addome jimmy davis un bambino di sei anni e lo nascose nei\n",
      "tre lettere sei cifre\n",
      "a anni da del mondo e con l'uscita al primo turno da sei slam\n",
      "sei denunce per furto d auto in due delle quali si dichiara\n",
      "a anni e sei mesi dalla\n",
      "sei cifre quello che mi serve\n",
      "a anni passa nelle giovanili del milan dove trascorre sei stagioni laureandosi per due volte\n",
      "sei cifre per la lettura\n",
      "a a sei\n",
      "sei cifre ecco qua\n",
      "a a sei minuti dal\n",
      "sei cifre come il codice a barre\n",
      "aa vv sei secoli di musica nel duomo\n",
      "quest uomo non si muove per meno di sei cifre\n",
      "a balaustra e sei gruppi di scalini che su ambedue i lati conducono verso la città\n",
      "quante sono le possibilita  di indovinare un pin di sei cifre\n",
      "a baltimora nel maryland il più giovane di sei figli\n",
      "penso di poterti far ottenere qualcosa sulla fascia bassa delle sei cifre\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "for i in range(10):\n",
    "    print(dataset_df.iloc[i]['sentence'])\n",
    "    print(dataset_df.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a al comma dopo le parole inferiore a sei</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a anni accoltellò all'addome jimmy davis un ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a anni da del mondo e con l'uscita al primo tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a anni e sei mesi dalla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a anni passa nelle giovanili del milan dove tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0          a al comma dopo le parole inferiore a sei      1\n",
       "1  a anni accoltellò all'addome jimmy davis un ba...      1\n",
       "2  a anni da del mondo e con l'uscita al primo tu...      1\n",
       "3                            a anni e sei mesi dalla      1\n",
       "4  a anni passa nelle giovanili del milan dove tr...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206694</th>\n",
       "      <td>sei cifre per la lettura</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206695</th>\n",
       "      <td>sei cifre quello che mi serve</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206696</th>\n",
       "      <td>sei denunce per furto d auto in due delle qual...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206697</th>\n",
       "      <td>tre lettere sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206698</th>\n",
       "      <td>tuttavia sono sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  class\n",
       "206694                           sei cifre per la lettura      0\n",
       "206695                      sei cifre quello che mi serve      0\n",
       "206696  sei denunce per furto d auto in due delle qual...      0\n",
       "206697                              tre lettere sei cifre      0\n",
       "206698                            tuttavia sono sei cifre      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    8.390515\n",
       "class       1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of words and mean\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    24\n",
       "class        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    3.59647\n",
       "class       0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the lengths\n",
    "%matplotlib inline\n",
    "\n",
    "length_sentence = dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1)\n",
    "plt.hist(length_sentence['sentence'],bins=range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di appena sei anni che</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sei un genio con questa roba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e sappiamo che sei stato paziente</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sei sempre stato pazzo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mike ross giocò da titolare al sei nazionie si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>era composto da sei corse che</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>repubblica austriaca di possedere un esercito ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>non ci credo che ti sei bevuto la storia del v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dopo sei mesi dall'entrata in servizio si ha d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sei un vero ottimista</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                             di appena sei anni che      1\n",
       "1                       sei un genio con questa roba      0\n",
       "2                  e sappiamo che sei stato paziente      0\n",
       "3                             sei sempre stato pazzo      0\n",
       "4  mike ross giocò da titolare al sei nazionie si...      1\n",
       "5                      era composto da sei corse che      1\n",
       "6  repubblica austriaca di possedere un esercito ...      1\n",
       "7  non ci credo che ti sei bevuto la storia del v...      0\n",
       "8  dopo sei mesi dall'entrata in servizio si ha d...      1\n",
       "9                              sei un vero ottimista      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i in range(dataset_df.shape[0])]\n",
    "random.shuffle(index)\n",
    "dataset = dataset_df.set_index([index]).sort_index()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>di appena sei anni che</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sei un genio con questa roba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e sappiamo che sei stato paziente</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sei sempre stato pazzo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mike ross giocò da titolare al sei nazionie si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>era composto da sei corse che</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>repubblica austriaca di possedere un esercito ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>non ci credo che ti sei bevuto la storia del v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dopo sei mesi dall entrata in servizio si ha d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sei un vero ottimista</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                             di appena sei anni che      1\n",
       "1                       sei un genio con questa roba      0\n",
       "2                  e sappiamo che sei stato paziente      0\n",
       "3                             sei sempre stato pazzo      0\n",
       "4  mike ross giocò da titolare al sei nazionie si...      1\n",
       "5                      era composto da sei corse che      1\n",
       "6  repubblica austriaca di possedere un esercito ...      1\n",
       "7  non ci credo che ti sei bevuto la storia del v...      0\n",
       "8  dopo sei mesi dall entrata in servizio si ha d...      1\n",
       "9                              sei un vero ottimista      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude1 = ['\\t', '\"', '?'] # list\n",
    "exclude2 = [\"'\", \"  \", \"   \", \"    \", \"     \"] # list\n",
    "\n",
    "def clean_text(text):\n",
    "    for c in exclude1:\n",
    "        text=text.replace(c,'')\n",
    "    for c in exclude2:\n",
    "        text=text.replace(c, \" \")\n",
    "    return text.lower().strip()\n",
    "\n",
    "sentence_processed = list(map(clean_text, dataset['sentence'].values))\n",
    "\n",
    "dataset['sentence'] = sentence_processed\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('it_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 52.5 ms, total: 15.7 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "def lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatizer = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatizer)\n",
    "\n",
    "sentence_processed = list(map(lemmatizer, dataset['sentence'].head(1000).values))\n",
    "\n",
    "#dataset['sentence'] = sentence_processed\n",
    "\n",
    "#dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.333333333333336"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(200000/1000)*13.3 / 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Tagger Classifier (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 175694\n",
      "Validation-Set size: 31005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset['sentence'], y=dataset['class']))\n",
    "train_index = splits[0][0]\n",
    "valid_index = splits[0][1]\n",
    "\n",
    "train_df = dataset.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "valid_df = dataset.loc[valid_index,:]\n",
    "print('Validation-Set size: %d' %len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "0    88103\n",
      "1    87961\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 50.04\n",
      "class 1 %: 49.96\n",
      "\n",
      "Validation Set\n",
      "0    15548\n",
      "1    15523\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 50.04\n",
      "class 1 %: 49.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Validation Set\")\n",
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(valid_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(valid_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsvfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_PATH, 'tsvfiles/train_data.tsv'), header=False, index=False, sep='\\t')\n",
    "valid_df.to_csv(os.path.join(ROOT_PATH, 'tsvfiles/valid_data.tsv'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vocabulary and Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('italian') + get_stop_words('english')\n",
    "\n",
    "my_stop_words = []\n",
    "for my_word in my_stop_words:\n",
    "    stop_words.append(my_word)\n",
    "    \n",
    "# Important step for this dataset!!!!\n",
    "stop_words.remove('sei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['criks', '16', '9', 'cv', 'ai']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = ['criks', 'crjis3','cr456is', '45crist','1v','f4','16','l','9','5ffff56566778','cv', 'ai']\n",
    "\n",
    "falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "[w for w in ww if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function returns FALSE if there is a digit in the string (i.e '4mmm', 'm44m', 'llp4')\n",
    "#falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "def get_vocab():\n",
    "    #allWords = []\n",
    "    vocab = set()\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "        #words_ = [w for w in words_only if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        #words_ = words_only\n",
    "        #allWords = allWords + words_\n",
    "        word_set = set(words_)\n",
    "        vocab.update(word_set)\n",
    "    \n",
    "    #vocab.remove('')\n",
    "    return list(vocab)#, allWords\n",
    "\n",
    "def get_all_words():\n",
    "    allWords = []\n",
    "    cnt = 0\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        # exclude words shorter than 2, but not numbers. exclude words with numbers inside, i.e. '3cris', 'c45ris', 'cris23'\n",
    "        #words_ = [w for w in words_only if (falseIfDigit(w) or w.isdigit()) and (len(w) > 1 or w.isdigit()) ]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        #words_ = words_only\n",
    "        allWords = allWords + words_\n",
    "        #word_set = set(words_)\n",
    "        cnt += 1\n",
    "        if cnt%10000==0:\n",
    "            print('-----------', cnt)\n",
    "    \n",
    "    return allWords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 64006\n",
      "CPU times: user 820 ms, sys: 2 µs, total: 820 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = get_vocab()\n",
    "print('--------------------', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 10000\n",
      "----------- 20000\n",
      "----------- 30000\n",
      "----------- 40000\n",
      "----------- 50000\n",
      "----------- 60000\n",
      "----------- 70000\n",
      "----------- 80000\n",
      "----------- 90000\n",
      "----------- 100000\n",
      "----------- 110000\n",
      "----------- 120000\n",
      "----------- 130000\n",
      "----------- 140000\n",
      "----------- 150000\n",
      "----------- 160000\n",
      "----------- 170000\n",
      "-------------------- 1522514\n",
      "CPU times: user 44min 42s, sys: 596 ms, total: 44min 43s\n",
      "Wall time: 44min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allWords = get_all_words()\n",
    "print('--------------------', len(allWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_allWords = Counter(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence = sorted(cnt_allWords.items(), key=lambda kv: len(vocab) - kv[1])\n",
    "#vocab_words_sorted_by_appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence_list = [word[0] for word in vocab_words_sorted_by_appearence]\n",
    "#vocab_words_sorted_by_appearence_list, len(vocab_words_sorted_by_appearence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab_words_sorted_by_appearence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64006\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = False\n",
    "REDUCED_SIZE_VOC = True\n",
    "SIZE_VOC = 25000\n",
    "\n",
    "vocab = vocab_words_sorted_by_appearence_list\n",
    "\n",
    "if STOP_WORDS:\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    words_and_frequence = [ (word, freq) for (word, freq) in vocab_words_sorted_by_appearence if word not in stop_words]\n",
    "\n",
    "print(len(vocab))\n",
    "if REDUCED_SIZE_VOC:\n",
    "    vocab = vocab[0:SIZE_VOC]\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ci' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abbiamo' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "PATH_VOC = os.path.join(ROOT_PATH, 'tsvfiles/vocab_5k.tsv')\n",
    "with open(PATH_VOC , 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/vocab_list.tsv', 'w') as file:\n",
    "    file.write(\"{}\\n\".format(PAD_WORD))\n",
    "    for word in vocab:\n",
    "        file.write(\"{}\\n\".format(word))\n",
    "        \n",
    "PATH_WORDS = os.path.join(ROOT_PATH, 'tsvfiles/n_words_5k.tsv')        \n",
    "with open(PATH_WORDS, 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/n_words.tsv', 'w') as file:\n",
    "    file.write(str(len(vocab)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
