{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\"> Generate a Dataset with Pandas </h1>\n",
    "\n",
    "\n",
    "pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "[Pandas Official Page](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dli/nlp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus_six_num: 103484, Corpus_six_ver: 285704, Ratio: 0.362\n",
      "CPU times: user 94 ms, sys: 6.16 ms, total: 100 ms\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_lines_num  = sum(1 for line in open(ROOT_PATH + '/data/sei_6/num/sei_num'))\n",
    "num_lines_ver  = sum(1 for line in open(ROOT_PATH + '/data/sei_6/ver/sei_verb'))\n",
    "\n",
    "ratio = num_lines_num  / float(num_lines_ver) \n",
    "print('Corpus_six_num: %d, Corpus_six_ver: %d, Ratio: %0.3f' %(num_lines_num, num_lines_ver, ratio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Data-Frame (Balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_1, directory_2):\n",
    "    # NOTE: Put in directory_2 the largest corpus\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"class\"] = []\n",
    "    l1 = 0\n",
    "    for file_path in os.listdir(directory_1):\n",
    "        with tf.gfile.GFile(os.path.join(directory_1 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines()]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(1)\n",
    "    \n",
    "    for file_path in os.listdir(directory_2):\n",
    "        with tf.gfile.GFile(os.path.join(directory_2 , file_path), \"rb\") as f:\n",
    "                # strip() removes white spaces before and after the string\n",
    "                # decode() converst a byte object ('b) in a python3 string\n",
    "                list_of_sentences = [s.strip().decode() for s in f.readlines() if np.random.random() <= ratio]\n",
    "                num_rows_1 = len(list_of_sentences)\n",
    "                for i in range(num_rows_1):\n",
    "                    data[\"sentence\"].append(list_of_sentences[i])\n",
    "                    data[\"class\"].append(0)\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 14.5 ms, total: 1.37 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_1 = ROOT_PATH + '/data/sei_6/num/'\n",
    "directory_2 = ROOT_PATH + '/data/sei_6/ver/'\n",
    "\n",
    "dataset_df = load_dataset(directory_1, directory_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "class          \n",
       "0        103891\n",
       "1        103484"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Balanced\n",
    "dataset_df.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a al comma dopo le parole inferiore a sei\n",
      "whoa whoa whoa sei cifre\n",
      "a anni accoltellò all'addome jimmy davis un bambino di sei anni e lo nascose nei\n",
      "warner bros ha comprato il mio libro per sei cifre\n",
      "a anni da del mondo e con l'uscita al primo turno da sei slam\n",
      "sono sei cifre\n",
      "a anni e sei mesi dalla\n",
      "sei denunce per furto d auto in due delle quali si dichiara\n",
      "a anni passa nelle giovanili del milan dove trascorre sei stagioni laureandosi per due volte\n",
      "sei cifre quello che mi serve\n",
      "a a sei\n",
      "sei cifre per la lettura\n",
      "a a sei minuti dal\n",
      "sei cifre non mi serviva altro\n",
      "aa vv sei secoli di musica nel duomo\n",
      "sei cifre in piu  del nostro ricevimento di nozze\n",
      "a balaustra e sei gruppi di scalini che su ambedue i lati conducono verso la città\n",
      "sei cifre ecco qua\n",
      "a baltimora nel maryland il più giovane di sei figli\n",
      "robin ha un tris di jack james ha poco meno di una coppia di sei\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "for i in range(10):\n",
    "    print(dataset_df.iloc[i]['sentence'])\n",
    "    print(dataset_df.iloc[-i -1]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a al comma dopo le parole inferiore a sei</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a anni accoltellò all'addome jimmy davis un ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a anni da del mondo e con l'uscita al primo tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a anni e sei mesi dalla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a anni passa nelle giovanili del milan dove tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0          a al comma dopo le parole inferiore a sei      1\n",
       "1  a anni accoltellò all'addome jimmy davis un ba...      1\n",
       "2  a anni da del mondo e con l'uscita al primo tu...      1\n",
       "3                            a anni e sei mesi dalla      1\n",
       "4  a anni passa nelle giovanili del milan dove tr...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207370</th>\n",
       "      <td>sei cifre quello che mi serve</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207371</th>\n",
       "      <td>sei denunce per furto d auto in due delle qual...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207372</th>\n",
       "      <td>sono sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207373</th>\n",
       "      <td>warner bros ha comprato il mio libro per sei c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207374</th>\n",
       "      <td>whoa whoa whoa sei cifre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  class\n",
       "207370                      sei cifre quello che mi serve      0\n",
       "207371  sei denunce per furto d auto in due delle qual...      0\n",
       "207372                                     sono sei cifre      0\n",
       "207373  warner bros ha comprato il mio libro per sei c...      0\n",
       "207374                           whoa whoa whoa sei cifre      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    8.390515\n",
       "class       1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of words and mean\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    24\n",
       "class        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    3.599363\n",
       "class       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length sentence\n",
    "dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the lengths\n",
    "%matplotlib inline\n",
    "\n",
    "length_sentence = dataset_df.astype('str').applymap(lambda x: str(x).count(' ') + 1)\n",
    "plt.hist(length_sentence['sentence'],bins=range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>è sul mercato da oltre sei mesi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>primitivo così i sei discendenti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sei anche un mercenario</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onestamente sara ripensando a quello che hai p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sei un agente validissimo pearce</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tu sei il superuomo nella base</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fratellino sei innamorato</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e poi quando sei nato charles ha detto che eri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hai guidato per sei stati nella direzione sbag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ci sei arrivata da sola</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                    è sul mercato da oltre sei mesi      1\n",
       "1                   primitivo così i sei discendenti      1\n",
       "2                            sei anche un mercenario      0\n",
       "3  onestamente sara ripensando a quello che hai p...      1\n",
       "4                   sei un agente validissimo pearce      0\n",
       "5                     tu sei il superuomo nella base      0\n",
       "6                          fratellino sei innamorato      0\n",
       "7  e poi quando sei nato charles ha detto che eri...      0\n",
       "8  hai guidato per sei stati nella direzione sbag...      1\n",
       "9                            ci sei arrivata da sola      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i in range(dataset_df.shape[0])]\n",
    "random.shuffle(index)\n",
    "dataset = dataset_df.set_index([index]).sort_index()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>è sul mercato da oltre sei mesi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>primitivo così i sei discendenti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sei anche un mercenario</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onestamente sara ripensando a quello che hai p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sei un agente validissimo pearce</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tu sei il superuomo nella base</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fratellino sei innamorato</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e poi quando sei nato charles ha detto che eri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hai guidato per sei stati nella direzione sbag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ci sei arrivata da sola</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  class\n",
       "0                    è sul mercato da oltre sei mesi      1\n",
       "1                   primitivo così i sei discendenti      1\n",
       "2                            sei anche un mercenario      0\n",
       "3  onestamente sara ripensando a quello che hai p...      1\n",
       "4                   sei un agente validissimo pearce      0\n",
       "5                     tu sei il superuomo nella base      0\n",
       "6                          fratellino sei innamorato      0\n",
       "7  e poi quando sei nato charles ha detto che eri...      0\n",
       "8  hai guidato per sei stati nella direzione sbag...      1\n",
       "9                            ci sei arrivata da sola      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude1 = ['\\t', '\"', '?'] # list\n",
    "exclude2 = [\"'\", \"  \", \"   \", \"    \", \"     \"] # list\n",
    "\n",
    "def clean_text(text):\n",
    "    for c in exclude1:\n",
    "        text=text.replace(c,'')\n",
    "    for c in exclude2:\n",
    "        text=text.replace(c, \" \")\n",
    "    return text.lower().strip()\n",
    "\n",
    "sentence_processed = list(map(clean_text, dataset['sentence'].values))\n",
    "\n",
    "dataset['sentence'] = sentence_processed\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Tagger Classifier (Train, Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Set size: 176268\n",
      "Validation-Set size: 31107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter =  model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=19850610)\n",
    "\n",
    "splits = list(splitter.split(X=dataset['sentence'], y=dataset['class']))\n",
    "train_index = splits[0][0]\n",
    "valid_index = splits[0][1]\n",
    "\n",
    "train_df = dataset.loc[train_index,:]\n",
    "print('Training-Set size: %d' %len(train_df))\n",
    "\n",
    "valid_df = dataset.loc[valid_index,:]\n",
    "print('Validation-Set size: %d' %len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "0    88103\n",
      "1    87961\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 50.04\n",
      "class 1 %: 49.96\n",
      "\n",
      "Validation Set\n",
      "0    15548\n",
      "1    15523\n",
      "Name: class, dtype: int64\n",
      "class 0 %: 50.04\n",
      "class 1 %: 49.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "training_value_counts = train_df['class'].value_counts()\n",
    "print(training_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(training_value_counts[0]/len(train_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(training_value_counts[1]/len(train_df)*100,2)))\n",
    "print(\"\")\n",
    "print(\"Validation Set\")\n",
    "validation_value_counts = valid_df['class'].value_counts()\n",
    "print(validation_value_counts)\n",
    "print(\"class 0 %: {}\".format(round(validation_value_counts[0]/len(valid_df)*100,2)))\n",
    "print(\"class 1 %: {}\".format(round(validation_value_counts[1]/len(valid_df)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tsvfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(ROOT_PATH, 'tsvfiles/train_data.tsv'), header=False, index=False, sep='\\t')\n",
    "valid_df.to_csv(os.path.join(ROOT_PATH, 'tsvfiles/valid_data.tsv'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vocabulary and Save it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words are commond words that if you want you can remove from your vocabulary (it is not mandatory but it could be a choice). The idea behind is the folloing. Suppose you want to build a topic detector, i.e. an algorithm that is able to identify the topic of a sentence. Is the sentence talking about history or physics? For this kind of task words like 'the', 'which', 'however' are not really important for the classification. We can choose to remove them and insert them into stop words (words that we do not put inside our vocabulary). On the contrary, words like 'centuries', 'battle' or 'gravity' and 'magnetism' must be present inside our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('italian') + get_stop_words('english')\n",
    "\n",
    "my_stop_words = []\n",
    "for my_word in my_stop_words:\n",
    "    stop_words.append(my_word)\n",
    "    \n",
    "# Important step for this dataset!!!!\n",
    "stop_words.remove('sei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function returns FALSE if there is a digit in the string (i.e '4mmm', 'm44m', 'llp4')\n",
    "#falseIfDigit = lambda word: not bool((re.match('^(?=.*[0-9])', str(word))))\n",
    "\n",
    "def get_vocab():\n",
    "    #allWords = []\n",
    "    vocab = set()\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        word_set = set(words_)\n",
    "        vocab.update(word_set)\n",
    "    \n",
    "    #vocab.remove('')\n",
    "    return list(vocab)\n",
    "\n",
    "def get_all_words():\n",
    "    allWords = []\n",
    "    cnt = 0\n",
    "    for text in train_df['sentence'].values:\n",
    "        words = text.split(' ')\n",
    "        # remove digits\n",
    "        words_only = [w for w in words if not w.isdigit()]\n",
    "        words_ = [w for w in words_only if len(w) > 0 ]\n",
    "        allWords = allWords + words_\n",
    "        cnt += 1\n",
    "        if cnt%10000==0:\n",
    "            print('-----------', cnt)\n",
    "    \n",
    "    return allWords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 64006\n",
      "CPU times: user 820 ms, sys: 2 µs, total: 820 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = get_vocab()\n",
    "print('--------------------', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 10000\n",
      "----------- 20000\n",
      "----------- 30000\n",
      "----------- 40000\n",
      "----------- 50000\n",
      "----------- 60000\n",
      "----------- 70000\n",
      "----------- 80000\n",
      "----------- 90000\n",
      "----------- 100000\n",
      "----------- 110000\n",
      "----------- 120000\n",
      "----------- 130000\n",
      "----------- 140000\n",
      "----------- 150000\n",
      "----------- 160000\n",
      "----------- 170000\n",
      "-------------------- 1522514\n",
      "CPU times: user 44min 42s, sys: 596 ms, total: 44min 43s\n",
      "Wall time: 44min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allWords = get_all_words()\n",
    "print('--------------------', len(allWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_allWords = Counter(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence = sorted(cnt_allWords.items(), key=lambda kv: len(vocab) - kv[1])\n",
    "#vocab_words_sorted_by_appearence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words_sorted_by_appearence_list = [word[0] for word in vocab_words_sorted_by_appearence]\n",
    "#vocab_words_sorted_by_appearence_list, len(vocab_words_sorted_by_appearence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64006\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = False\n",
    "REDUCED_SIZE_VOC = True\n",
    "SIZE_VOC = 25000\n",
    "\n",
    "vocab = vocab_words_sorted_by_appearence_list\n",
    "\n",
    "if STOP_WORDS:\n",
    "    vocab = [w for w in vocab if w not in stop_words]\n",
    "    words_and_frequence = [ (word, freq) for (word, freq) in vocab_words_sorted_by_appearence if word not in stop_words]\n",
    "\n",
    "print(len(vocab))\n",
    "if REDUCED_SIZE_VOC:\n",
    "    vocab = vocab[0:SIZE_VOC]\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = '#=KS=#'\n",
    "\n",
    "PATH_VOC = os.path.join(ROOT_PATH, 'tsvfiles/vocab_5k.tsv')\n",
    "with open(PATH_VOC , 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/vocab_list.tsv', 'w') as file:\n",
    "    file.write(\"{}\\n\".format(PAD_WORD))\n",
    "    for word in vocab:\n",
    "        file.write(\"{}\\n\".format(word))\n",
    "        \n",
    "PATH_WORDS = os.path.join(ROOT_PATH, 'tsvfiles/n_words_5k.tsv')        \n",
    "with open(PATH_WORDS, 'w') as file:\n",
    "#with open('/home/asr/Data/classif_task/jsgf_data/n_words.tsv', 'w') as file:\n",
    "    file.write(str(len(vocab)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
